## Group project: Real or Not? NLP with Disaster Tweets

![Banner](https://raw.githubusercontent.com/mbayle98/DMML2020-Alpina/main/Documents/Banner_project_alpina.png)


### ğŸ•µï¸ Project description

Twitter has become a popular social network for many users. However, it is also used to relay fake news.  

The goal of this project is to build a machine learning model that predicts which Tweets are about real disasters and which oneâ€™s arenâ€™t.  

To train the model, we have access to a dataset of 6,471 Tweets that were already classified (0 = fake news, 1 = real news). The aim is to use the different techniques seen in class to perform the EDA and the data cleaning in a first step before testing different classifiers to determine which one provides the best accuracy.  

### ğŸš€ Progress of the project

#### Week 1

ğŸ’» **Github**

- Repo initialisation
- Setting up the repo structure
- Ressources uploaded: training_set, test_set
- Draft README

ğŸ’¾ **Data**

- Import the inital database into the notebook
- Calculate the base rate on the original dataset
- Distribution of the data in the inital database: 3,701 fake news & 2,770 real news
- Train the model on the original dataset without any modification to set the base benchmark accuracy (basic model)

ğŸ§¹ **Text preprocessing**

- Word cloud of the dataset to determine the type of words contained in the Tweets
- Preproceesing the text to remove irrevelant information 
- Run an elementar model

#### Week 2

ğŸ“Š **EDA**

- Plot the Top 50 keywords (entire training set, distinction between fake/real news Tweets)
- Plot the Top 50 locations (entire training set, distinction between fake/real news Tweets)
- Plot the most common words and word types in Tweets

#### Week 3

ğŸ“Š **EDA** + âš™ï¸ **Optimization**

- EDA completed
- Cleaning the features in progress
- Testing different classifers
- Cross validation to be done

#### Week 4

âš™ï¸ **Optimization**

- Different classifiers tested: Logistic regression, kNNeighbours, Decision Tree, Random Forest
- Hyper-parameter optimization with GridSearch

### ğŸ¥‡ Results

10 subsmissions on AIcrowd:
- 1<sup>st</sup> submission (Logistic regression optimized numbers removal model): 0.818
- 2<sup>st</sup> submission (Logistic regression optimized model): 0.816
- 3<sup>st</sup> submission (Logistic regression model): 0.811
- 4<sup>st</sup> submission (basic model): 0.57

![Graphic AIcrowd](Documents/AICROWD.PNG)

### ğŸ“¹ Video

To be added later ğŸ˜
