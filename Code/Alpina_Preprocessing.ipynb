{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALPINA_PREPROCESSING_WIP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnNsAJ59BSMjpKioBlQW3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbayle98/DMML2020-Alpina/blob/main/ALPINA_PREPROCESSING_WIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSrIvNn5MPUX"
      },
      "source": [
        "# **Group Project: Real or Not? NLP with Disaster Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeZJluPrMTn3"
      },
      "source": [
        "![Banner](https://drive.google.com/uc?id=1lxLXbvJfi4gFigF7ajX8hV1fiL3fQW49)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtEpbh9MYkf"
      },
      "source": [
        "### ðŸ”— **[Github Repo](https://github.com/mbayle98/DMML2020-Alpina)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kuz0HWMc8h"
      },
      "source": [
        "### ðŸ“¦ **Import Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcxPrVQUMtxK",
        "outputId": "410eb802-77ae-4e1b-b0cc-a6a1c92904fa"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stpw\n",
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytSUpv5FMEl7",
        "outputId": "64869595-7da1-46ca-d4fb-68cef6ae9e56"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "stop = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN7YVet_NScx"
      },
      "source": [
        "# Remove :// chain\n",
        "\n",
        "def bin_http(tweet):\n",
        "    tweet = ' '.join(re.sub(\"\\w+:\\/\\/\\S+\",\" \",tweet).split())\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOO7d0nnNpkf"
      },
      "source": [
        "# English contraction\n",
        "\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "    \n",
        "  return contractions_re.sub(replace, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWRBy8yfNsIy"
      },
      "source": [
        "# Remove username\n",
        "\n",
        "def bin_username(tweet):\n",
        "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+) | (@ [A-Za-z0-9]+) \",\" \", tweet).split())\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIjh-37iPF9R"
      },
      "source": [
        "# Remove special characters\n",
        "\n",
        "def bin_spe(tweet):\n",
        "    tweet = ' '.join(re.sub(\"[^0-9A-Za-z \\t]\",\" \", tweet).split())\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt_GBnd8QFay"
      },
      "source": [
        "# Definition lemmatizer\n",
        "\n",
        "def lemmatizer(text):        \n",
        "    sent = []\n",
        "    doc = nlp(text)\n",
        "    for word in doc:\n",
        "        sent.append(word.lemma_)\n",
        "    return \" \".join(sent)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
